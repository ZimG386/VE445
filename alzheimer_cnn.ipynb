{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Any Reason for this?\n",
    "])\n",
    "\n",
    "data=ImageFolder(\"./archive/Dataset/\", transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# c0=0    # mild\n",
    "# c1=0    # moderate\n",
    "# c2=0    # non\n",
    "# c3=0    # very mild\n",
    "# for i,d in enumerate(data):\n",
    "#     if data[i][1]==0:   c0+=1\n",
    "#     elif data[i][1]==1: c1+=1\n",
    "#     elif data[i][1]==2: c2+=1\n",
    "#     elif data[i][1]==3: c3+=1\n",
    "# print(c0,c1,c2,c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n=len(data)\n",
    "n_test=int(0.2*n)   # 20% for test\n",
    "train_data,test_data=random_split(data,[n-n_test,n_test],torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# c0=0    # mild\n",
    "# c1=0    # moderate\n",
    "# c2=0    # non\n",
    "# c3=0    # very mild\n",
    "# for i,d in enumerate(test_data):\n",
    "#     if test_data[i][1]==0:   c0+=1\n",
    "#     elif test_data[i][1]==1: c1+=1\n",
    "#     elif test_data[i][1]==2: c2+=1\n",
    "#     elif test_data[i][1]==3: c3+=1\n",
    "# print(c0,c1,c2,c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainloader=DataLoader(train_data,batch_size=128,drop_last=False,shuffle=True)\n",
    "testloader=DataLoader(test_data,batch_size=128,drop_last=False,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for i,data in enumerate(trainloader):\n",
    "#     imgs, targets=data\n",
    "#     if i<10: print(imgs.shape)\n",
    "#     else: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# fig=Image.open(\"./archive/Dataset/Mild_Demented/mild.jpg\")\n",
    "# plt.imshow(fig)\n",
    "# print(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # Set for convolution operation\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 16, 3, padding=1), \n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.dp = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128*8*8, 32),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = torch.nn.Linear(32, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Three-layer convolutional network (Conv -> ReLU -> MaxPool)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.dp(x)\n",
    "        x = x.view(-1, 128*8*8)\n",
    "        x = self.fc1(x) # Fully connected layer -> ReLU\n",
    "        x = self.fc2(x) \n",
    "        out = F.log_softmax(x, dim=1) # Softmax probability\n",
    "        return out\n",
    "\n",
    "net_cpu = ConvNet()\n",
    "net_gpu = net_cpu.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [448/5000] Loss: 1.234264\n",
      "Train Epoch: 0 [960/5000] Loss: 1.192123\n",
      "Train Epoch: 0 [1472/5000] Loss: 1.028847\n",
      "Train Epoch: 0 [1984/5000] Loss: 1.158864\n",
      "Train Epoch: 0 [2496/5000] Loss: 0.968348\n",
      "1.176887346804142\n",
      "Test set: Average loss: 0.0010, Accuracy: 651/10000 (50%)\n",
      "Train Epoch: 1 [448/5000] Loss: 1.055168\n",
      "Train Epoch: 1 [960/5000] Loss: 0.980253\n",
      "Train Epoch: 1 [1472/5000] Loss: 0.993573\n",
      "Train Epoch: 1 [1984/5000] Loss: 1.018553\n",
      "Train Epoch: 1 [2496/5000] Loss: 0.973411\n",
      "1.0067195862531662\n",
      "Test set: Average loss: 0.0010, Accuracy: 656/10000 (51%)\n",
      "Train Epoch: 2 [448/5000] Loss: 0.977366\n",
      "Train Epoch: 2 [960/5000] Loss: 1.022532\n",
      "Train Epoch: 2 [1472/5000] Loss: 0.884617\n",
      "Train Epoch: 2 [1984/5000] Loss: 1.027266\n",
      "Train Epoch: 2 [2496/5000] Loss: 0.961592\n",
      "0.9522234946489334\n",
      "Test set: Average loss: 0.0009, Accuracy: 702/10000 (54%)\n",
      "Train Epoch: 3 [448/5000] Loss: 0.939151\n",
      "Train Epoch: 3 [960/5000] Loss: 0.932697\n",
      "Train Epoch: 3 [1472/5000] Loss: 0.946233\n",
      "Train Epoch: 3 [1984/5000] Loss: 0.860118\n",
      "Train Epoch: 3 [2496/5000] Loss: 0.873317\n",
      "0.9258226558566094\n",
      "Test set: Average loss: 0.0009, Accuracy: 670/10000 (52%)\n",
      "Train Epoch: 4 [448/5000] Loss: 0.777402\n",
      "Train Epoch: 4 [960/5000] Loss: 0.938945\n",
      "Train Epoch: 4 [1472/5000] Loss: 1.006959\n",
      "Train Epoch: 4 [1984/5000] Loss: 0.916795\n",
      "Train Epoch: 4 [2496/5000] Loss: 0.890977\n",
      "0.9201558992266655\n",
      "Test set: Average loss: 0.0009, Accuracy: 737/10000 (57%)\n",
      "Train Epoch: 5 [448/5000] Loss: 0.907668\n",
      "Train Epoch: 5 [960/5000] Loss: 0.877484\n",
      "Train Epoch: 5 [1472/5000] Loss: 0.936767\n",
      "Train Epoch: 5 [1984/5000] Loss: 0.973712\n",
      "Train Epoch: 5 [2496/5000] Loss: 0.879805\n",
      "0.8996330931782722\n",
      "Test set: Average loss: 0.0010, Accuracy: 637/10000 (49%)\n",
      "Train Epoch: 6 [448/5000] Loss: 0.909240\n",
      "Train Epoch: 6 [960/5000] Loss: 0.852635\n",
      "Train Epoch: 6 [1472/5000] Loss: 0.936893\n",
      "Train Epoch: 6 [1984/5000] Loss: 0.839148\n",
      "Train Epoch: 6 [2496/5000] Loss: 0.933353\n",
      "0.9009059056639671\n",
      "Test set: Average loss: 0.0009, Accuracy: 757/10000 (59%)\n",
      "Train Epoch: 7 [448/5000] Loss: 0.957539\n",
      "Train Epoch: 7 [960/5000] Loss: 0.906467\n",
      "Train Epoch: 7 [1472/5000] Loss: 0.893200\n",
      "Train Epoch: 7 [1984/5000] Loss: 0.855887\n",
      "Train Epoch: 7 [2496/5000] Loss: 0.884819\n",
      "0.8666952133178711\n",
      "Test set: Average loss: 0.0009, Accuracy: 760/10000 (59%)\n",
      "Train Epoch: 8 [448/5000] Loss: 0.750773\n",
      "Train Epoch: 8 [960/5000] Loss: 0.957538\n",
      "Train Epoch: 8 [1472/5000] Loss: 0.809177\n",
      "Train Epoch: 8 [1984/5000] Loss: 0.844509\n",
      "Train Epoch: 8 [2496/5000] Loss: 0.952795\n",
      "0.8555545806884766\n",
      "Test set: Average loss: 0.0009, Accuracy: 760/10000 (59%)\n",
      "Train Epoch: 9 [448/5000] Loss: 0.792276\n",
      "Train Epoch: 9 [960/5000] Loss: 0.868337\n",
      "Train Epoch: 9 [1472/5000] Loss: 0.800438\n",
      "Train Epoch: 9 [1984/5000] Loss: 0.810928\n",
      "Train Epoch: 9 [2496/5000] Loss: 0.875803\n",
      "0.8360996454954147\n",
      "Test set: Average loss: 0.0009, Accuracy: 741/10000 (57%)\n",
      "Train Epoch: 10 [448/5000] Loss: 0.825281\n",
      "Train Epoch: 10 [960/5000] Loss: 0.803332\n",
      "Train Epoch: 10 [1472/5000] Loss: 0.848510\n",
      "Train Epoch: 10 [1984/5000] Loss: 0.935005\n",
      "Train Epoch: 10 [2496/5000] Loss: 0.832994\n",
      "0.826998870074749\n",
      "Test set: Average loss: 0.0009, Accuracy: 776/10000 (60%)\n",
      "Train Epoch: 11 [448/5000] Loss: 0.832698\n",
      "Train Epoch: 11 [960/5000] Loss: 0.820723\n",
      "Train Epoch: 11 [1472/5000] Loss: 0.739659\n",
      "Train Epoch: 11 [1984/5000] Loss: 0.850299\n",
      "Train Epoch: 11 [2496/5000] Loss: 0.760378\n",
      "0.811349019408226\n",
      "Test set: Average loss: 0.0008, Accuracy: 812/10000 (63%)\n",
      "Train Epoch: 12 [448/5000] Loss: 0.750534\n",
      "Train Epoch: 12 [960/5000] Loss: 0.716712\n",
      "Train Epoch: 12 [1472/5000] Loss: 0.690921\n",
      "Train Epoch: 12 [1984/5000] Loss: 0.635195\n",
      "Train Epoch: 12 [2496/5000] Loss: 0.769534\n",
      "0.7572867766022682\n",
      "Test set: Average loss: 0.0008, Accuracy: 846/10000 (66%)\n",
      "Train Epoch: 13 [448/5000] Loss: 0.754180\n",
      "Train Epoch: 13 [960/5000] Loss: 0.734480\n",
      "Train Epoch: 13 [1472/5000] Loss: 0.622708\n",
      "Train Epoch: 13 [1984/5000] Loss: 0.744337\n",
      "Train Epoch: 13 [2496/5000] Loss: 0.707321\n",
      "0.7231426939368248\n",
      "Test set: Average loss: 0.0007, Accuracy: 900/10000 (70%)\n",
      "Train Epoch: 14 [448/5000] Loss: 0.734310\n",
      "Train Epoch: 14 [960/5000] Loss: 0.678680\n",
      "Train Epoch: 14 [1472/5000] Loss: 0.551226\n",
      "Train Epoch: 14 [1984/5000] Loss: 0.693768\n",
      "Train Epoch: 14 [2496/5000] Loss: 0.635008\n",
      "0.6345520570874215\n",
      "Test set: Average loss: 0.0006, Accuracy: 941/10000 (73%)\n",
      "Train Epoch: 15 [448/5000] Loss: 0.483630\n",
      "Train Epoch: 15 [960/5000] Loss: 0.609912\n",
      "Train Epoch: 15 [1472/5000] Loss: 0.590572\n",
      "Train Epoch: 15 [1984/5000] Loss: 0.676287\n",
      "Train Epoch: 15 [2496/5000] Loss: 0.551793\n",
      "0.5830947093665599\n",
      "Test set: Average loss: 0.0006, Accuracy: 968/10000 (75%)\n",
      "Train Epoch: 16 [448/5000] Loss: 0.508013\n",
      "Train Epoch: 16 [960/5000] Loss: 0.477788\n",
      "Train Epoch: 16 [1472/5000] Loss: 0.372923\n",
      "Train Epoch: 16 [1984/5000] Loss: 0.480078\n",
      "Train Epoch: 16 [2496/5000] Loss: 0.454914\n",
      "0.504715372622013\n",
      "Test set: Average loss: 0.0005, Accuracy: 1040/10000 (81%)\n",
      "Train Epoch: 17 [448/5000] Loss: 0.398368\n",
      "Train Epoch: 17 [960/5000] Loss: 0.435356\n",
      "Train Epoch: 17 [1472/5000] Loss: 0.443595\n",
      "Train Epoch: 17 [1984/5000] Loss: 0.392702\n",
      "Train Epoch: 17 [2496/5000] Loss: 0.504588\n",
      "0.4229027919471264\n",
      "Test set: Average loss: 0.0005, Accuracy: 1040/10000 (81%)\n",
      "Train Epoch: 18 [448/5000] Loss: 0.319731\n",
      "Train Epoch: 18 [960/5000] Loss: 0.348549\n",
      "Train Epoch: 18 [1472/5000] Loss: 0.395762\n",
      "Train Epoch: 18 [1984/5000] Loss: 0.452708\n",
      "Train Epoch: 18 [2496/5000] Loss: 0.358595\n",
      "0.36359197050333025\n",
      "Test set: Average loss: 0.0004, Accuracy: 1103/10000 (86%)\n",
      "Train Epoch: 19 [448/5000] Loss: 0.450720\n",
      "Train Epoch: 19 [960/5000] Loss: 0.271245\n",
      "Train Epoch: 19 [1472/5000] Loss: 0.345136\n",
      "Train Epoch: 19 [1984/5000] Loss: 0.289784\n",
      "Train Epoch: 19 [2496/5000] Loss: 0.344468\n",
      "0.32578014098107816\n",
      "Test set: Average loss: 0.0003, Accuracy: 1107/10000 (86%)\n",
      "Train Epoch: 20 [448/5000] Loss: 0.280024\n",
      "Train Epoch: 20 [960/5000] Loss: 0.309936\n",
      "Train Epoch: 20 [1472/5000] Loss: 0.269713\n",
      "Train Epoch: 20 [1984/5000] Loss: 0.273366\n",
      "Train Epoch: 20 [2496/5000] Loss: 0.310289\n",
      "0.26586394011974335\n",
      "Test set: Average loss: 0.0004, Accuracy: 1109/10000 (86%)\n",
      "Train Epoch: 21 [448/5000] Loss: 0.367013\n",
      "Train Epoch: 21 [960/5000] Loss: 0.224212\n",
      "Train Epoch: 21 [1472/5000] Loss: 0.163470\n",
      "Train Epoch: 21 [1984/5000] Loss: 0.330192\n",
      "Train Epoch: 21 [2496/5000] Loss: 0.309836\n",
      "0.2253912039101124\n",
      "Test set: Average loss: 0.0002, Accuracy: 1181/10000 (92%)\n",
      "Train Epoch: 22 [448/5000] Loss: 0.173861\n",
      "Train Epoch: 22 [960/5000] Loss: 0.186253\n",
      "Train Epoch: 22 [1472/5000] Loss: 0.135730\n",
      "Train Epoch: 22 [1984/5000] Loss: 0.209299\n",
      "Train Epoch: 22 [2496/5000] Loss: 0.179331\n",
      "0.1902927180752158\n",
      "Test set: Average loss: 0.0002, Accuracy: 1190/10000 (92%)\n",
      "Train Epoch: 23 [448/5000] Loss: 0.161341\n",
      "Train Epoch: 23 [960/5000] Loss: 0.158263\n",
      "Train Epoch: 23 [1472/5000] Loss: 0.096367\n",
      "Train Epoch: 23 [1984/5000] Loss: 0.127988\n",
      "Train Epoch: 23 [2496/5000] Loss: 0.142110\n",
      "0.14379832353442906\n",
      "Test set: Average loss: 0.0002, Accuracy: 1191/10000 (93%)\n",
      "Train Epoch: 24 [448/5000] Loss: 0.110942\n",
      "Train Epoch: 24 [960/5000] Loss: 0.094547\n",
      "Train Epoch: 24 [1472/5000] Loss: 0.108469\n",
      "Train Epoch: 24 [1984/5000] Loss: 0.120813\n",
      "Train Epoch: 24 [2496/5000] Loss: 0.167865\n",
      "0.14141426105052232\n",
      "Test set: Average loss: 0.0002, Accuracy: 1177/10000 (91%)\n",
      "Train Epoch: 25 [448/5000] Loss: 0.100436\n",
      "Train Epoch: 25 [960/5000] Loss: 0.111592\n",
      "Train Epoch: 25 [1472/5000] Loss: 0.074073\n",
      "Train Epoch: 25 [1984/5000] Loss: 0.155453\n",
      "Train Epoch: 25 [2496/5000] Loss: 0.091681\n",
      "0.11854634564369917\n",
      "Test set: Average loss: 0.0002, Accuracy: 1206/10000 (94%)\n",
      "Train Epoch: 26 [448/5000] Loss: 0.043842\n",
      "Train Epoch: 26 [960/5000] Loss: 0.079522\n",
      "Train Epoch: 26 [1472/5000] Loss: 0.084302\n",
      "Train Epoch: 26 [1984/5000] Loss: 0.116561\n",
      "Train Epoch: 26 [2496/5000] Loss: 0.082015\n",
      "0.09839530503377318\n",
      "Test set: Average loss: 0.0002, Accuracy: 1207/10000 (94%)\n",
      "Train Epoch: 27 [448/5000] Loss: 0.087024\n",
      "Train Epoch: 27 [960/5000] Loss: 0.101705\n",
      "Train Epoch: 27 [1472/5000] Loss: 0.033478\n",
      "Train Epoch: 27 [1984/5000] Loss: 0.103518\n",
      "Train Epoch: 27 [2496/5000] Loss: 0.079083\n",
      "0.09506505713798105\n",
      "Test set: Average loss: 0.0002, Accuracy: 1206/10000 (94%)\n",
      "Train Epoch: 28 [448/5000] Loss: 0.143516\n",
      "Train Epoch: 28 [960/5000] Loss: 0.082981\n",
      "Train Epoch: 28 [1472/5000] Loss: 0.082114\n",
      "Train Epoch: 28 [1984/5000] Loss: 0.147961\n",
      "Train Epoch: 28 [2496/5000] Loss: 0.097162\n",
      "0.09331057369709014\n",
      "Test set: Average loss: 0.0001, Accuracy: 1239/10000 (96%)\n",
      "Train Epoch: 29 [448/5000] Loss: 0.060058\n",
      "Train Epoch: 29 [960/5000] Loss: 0.116239\n",
      "Train Epoch: 29 [1472/5000] Loss: 0.077780\n",
      "Train Epoch: 29 [1984/5000] Loss: 0.058171\n",
      "Train Epoch: 29 [2496/5000] Loss: 0.095297\n",
      "0.07924023657105864\n",
      "Test set: Average loss: 0.0001, Accuracy: 1231/10000 (96%)\n",
      "Train Epoch: 30 [448/5000] Loss: 0.059153\n",
      "Train Epoch: 30 [960/5000] Loss: 0.046657\n",
      "Train Epoch: 30 [1472/5000] Loss: 0.081288\n",
      "Train Epoch: 30 [1984/5000] Loss: 0.118025\n",
      "Train Epoch: 30 [2496/5000] Loss: 0.087080\n",
      "0.0692436930257827\n",
      "Test set: Average loss: 0.0001, Accuracy: 1224/10000 (95%)\n",
      "Train Epoch: 31 [448/5000] Loss: 0.051034\n",
      "Train Epoch: 31 [960/5000] Loss: 0.085172\n",
      "Train Epoch: 31 [1472/5000] Loss: 0.030492\n",
      "Train Epoch: 31 [1984/5000] Loss: 0.104602\n",
      "Train Epoch: 31 [2496/5000] Loss: 0.098336\n",
      "0.06956297219730914\n",
      "Test set: Average loss: 0.0001, Accuracy: 1238/10000 (96%)\n",
      "Train Epoch: 32 [448/5000] Loss: 0.072822\n",
      "Train Epoch: 32 [960/5000] Loss: 0.005907\n",
      "Train Epoch: 32 [1472/5000] Loss: 0.063191\n",
      "Train Epoch: 32 [1984/5000] Loss: 0.051788\n",
      "Train Epoch: 32 [2496/5000] Loss: 0.028525\n",
      "0.0600310274399817\n",
      "Test set: Average loss: 0.0001, Accuracy: 1249/10000 (97%)\n",
      "Train Epoch: 33 [448/5000] Loss: 0.040661\n",
      "Train Epoch: 33 [960/5000] Loss: 0.030763\n",
      "Train Epoch: 33 [1472/5000] Loss: 0.109777\n",
      "Train Epoch: 33 [1984/5000] Loss: 0.016732\n",
      "Train Epoch: 33 [2496/5000] Loss: 0.043012\n",
      "0.059706470975652334\n",
      "Test set: Average loss: 0.0001, Accuracy: 1242/10000 (97%)\n",
      "Train Epoch: 34 [448/5000] Loss: 0.028411\n",
      "Train Epoch: 34 [960/5000] Loss: 0.010468\n",
      "Train Epoch: 34 [1472/5000] Loss: 0.059043\n",
      "Train Epoch: 34 [1984/5000] Loss: 0.102021\n",
      "Train Epoch: 34 [2496/5000] Loss: 0.041372\n",
      "0.043871473707258704\n",
      "Test set: Average loss: 0.0001, Accuracy: 1242/10000 (97%)\n",
      "Train Epoch: 35 [448/5000] Loss: 0.025873\n",
      "Train Epoch: 35 [960/5000] Loss: 0.036909\n",
      "Train Epoch: 35 [1472/5000] Loss: 0.044132\n",
      "Train Epoch: 35 [1984/5000] Loss: 0.014974\n",
      "Train Epoch: 35 [2496/5000] Loss: 0.061383\n",
      "0.045211103605106474\n",
      "Test set: Average loss: 0.0001, Accuracy: 1252/10000 (97%)\n",
      "Train Epoch: 36 [448/5000] Loss: 0.007037\n",
      "Train Epoch: 36 [960/5000] Loss: 0.075854\n",
      "Train Epoch: 36 [1472/5000] Loss: 0.026957\n",
      "Train Epoch: 36 [1984/5000] Loss: 0.030669\n",
      "Train Epoch: 36 [2496/5000] Loss: 0.022871\n",
      "0.03289215210825205\n",
      "Test set: Average loss: 0.0001, Accuracy: 1243/10000 (97%)\n",
      "Train Epoch: 37 [448/5000] Loss: 0.045723\n",
      "Train Epoch: 37 [960/5000] Loss: 0.058530\n",
      "Train Epoch: 37 [1472/5000] Loss: 0.046558\n",
      "Train Epoch: 37 [1984/5000] Loss: 0.085211\n",
      "Train Epoch: 37 [2496/5000] Loss: 0.011027\n",
      "0.038931540213525295\n",
      "Test set: Average loss: 0.0001, Accuracy: 1251/10000 (97%)\n",
      "Train Epoch: 38 [448/5000] Loss: 0.022292\n",
      "Train Epoch: 38 [960/5000] Loss: 0.009152\n",
      "Train Epoch: 38 [1472/5000] Loss: 0.068549\n",
      "Train Epoch: 38 [1984/5000] Loss: 0.021246\n",
      "Train Epoch: 38 [2496/5000] Loss: 0.020517\n",
      "0.040106918173842133\n",
      "Test set: Average loss: 0.0001, Accuracy: 1244/10000 (97%)\n",
      "Train Epoch: 39 [448/5000] Loss: 0.008950\n",
      "Train Epoch: 39 [960/5000] Loss: 0.031474\n",
      "Train Epoch: 39 [1472/5000] Loss: 0.077165\n",
      "Train Epoch: 39 [1984/5000] Loss: 0.028147\n",
      "Train Epoch: 39 [2496/5000] Loss: 0.074615\n",
      "0.039570213132537904\n",
      "Test set: Average loss: 0.0001, Accuracy: 1251/10000 (97%)\n",
      "Train Epoch: 40 [448/5000] Loss: 0.029533\n",
      "Train Epoch: 40 [960/5000] Loss: 0.055235\n",
      "Train Epoch: 40 [1472/5000] Loss: 0.039130\n",
      "Train Epoch: 40 [1984/5000] Loss: 0.040581\n",
      "Train Epoch: 40 [2496/5000] Loss: 0.036086\n",
      "0.03883770573884249\n",
      "Test set: Average loss: 0.0001, Accuracy: 1250/10000 (97%)\n",
      "Train Epoch: 41 [448/5000] Loss: 0.005679\n",
      "Train Epoch: 41 [960/5000] Loss: 0.050045\n",
      "Train Epoch: 41 [1472/5000] Loss: 0.026413\n",
      "Train Epoch: 41 [1984/5000] Loss: 0.041197\n",
      "Train Epoch: 41 [2496/5000] Loss: 0.041869\n",
      "0.03722620097687468\n",
      "Test set: Average loss: 0.0001, Accuracy: 1252/10000 (97%)\n",
      "Train Epoch: 42 [448/5000] Loss: 0.021655\n",
      "Train Epoch: 42 [960/5000] Loss: 0.013844\n",
      "Train Epoch: 42 [1472/5000] Loss: 0.027733\n",
      "Train Epoch: 42 [1984/5000] Loss: 0.008161\n",
      "Train Epoch: 42 [2496/5000] Loss: 0.033062\n",
      "0.031211514491587877\n",
      "Test set: Average loss: 0.0001, Accuracy: 1242/10000 (97%)\n",
      "Train Epoch: 43 [448/5000] Loss: 0.019223\n",
      "Train Epoch: 43 [960/5000] Loss: 0.015118\n",
      "Train Epoch: 43 [1472/5000] Loss: 0.021714\n",
      "Train Epoch: 43 [1984/5000] Loss: 0.095972\n",
      "Train Epoch: 43 [2496/5000] Loss: 0.014421\n",
      "0.03802575923036784\n",
      "Test set: Average loss: 0.0001, Accuracy: 1249/10000 (97%)\n",
      "Train Epoch: 44 [448/5000] Loss: 0.053302\n",
      "Train Epoch: 44 [960/5000] Loss: 0.015452\n",
      "Train Epoch: 44 [1472/5000] Loss: 0.049132\n",
      "Train Epoch: 44 [1984/5000] Loss: 0.028816\n",
      "Train Epoch: 44 [2496/5000] Loss: 0.033576\n",
      "0.04524776458274573\n",
      "Test set: Average loss: 0.0001, Accuracy: 1256/10000 (98%)\n",
      "Train Epoch: 45 [448/5000] Loss: 0.060601\n",
      "Train Epoch: 45 [960/5000] Loss: 0.023622\n",
      "Train Epoch: 45 [1472/5000] Loss: 0.006234\n",
      "Train Epoch: 45 [1984/5000] Loss: 0.040829\n",
      "Train Epoch: 45 [2496/5000] Loss: 0.044075\n",
      "0.02673335822764784\n",
      "Test set: Average loss: 0.0001, Accuracy: 1252/10000 (97%)\n",
      "Train Epoch: 46 [448/5000] Loss: 0.057646\n",
      "Train Epoch: 46 [960/5000] Loss: 0.032832\n",
      "Train Epoch: 46 [1472/5000] Loss: 0.020663\n",
      "Train Epoch: 46 [1984/5000] Loss: 0.010912\n",
      "Train Epoch: 46 [2496/5000] Loss: 0.057533\n",
      "0.027547846478410066\n",
      "Test set: Average loss: 0.0001, Accuracy: 1255/10000 (98%)\n",
      "Train Epoch: 47 [448/5000] Loss: 0.015542\n",
      "Train Epoch: 47 [960/5000] Loss: 0.087516\n",
      "Train Epoch: 47 [1472/5000] Loss: 0.037386\n",
      "Train Epoch: 47 [1984/5000] Loss: 0.044959\n",
      "Train Epoch: 47 [2496/5000] Loss: 0.049140\n",
      "0.032653901563026014\n",
      "Test set: Average loss: 0.0001, Accuracy: 1242/10000 (97%)\n",
      "Train Epoch: 48 [448/5000] Loss: 0.031235\n",
      "Train Epoch: 48 [960/5000] Loss: 0.057731\n",
      "Train Epoch: 48 [1472/5000] Loss: 0.023574\n",
      "Train Epoch: 48 [1984/5000] Loss: 0.051722\n",
      "Train Epoch: 48 [2496/5000] Loss: 0.020835\n",
      "0.029008962900843472\n",
      "Test set: Average loss: 0.0001, Accuracy: 1249/10000 (97%)\n",
      "Train Epoch: 49 [448/5000] Loss: 0.017079\n",
      "Train Epoch: 49 [960/5000] Loss: 0.007227\n",
      "Train Epoch: 49 [1472/5000] Loss: 0.009994\n",
      "Train Epoch: 49 [1984/5000] Loss: 0.046724\n",
      "Train Epoch: 49 [2496/5000] Loss: 0.027271\n",
      "0.029323481302708388\n",
      "Test set: Average loss: 0.0001, Accuracy: 1258/10000 (98%)\n",
      "Train and predict complete!\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "summaryWriter = SummaryWriter(\"logs/lyf_cnn_3\")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    net_gpu.parameters(),\n",
    "    lr = 0.001,\n",
    "    betas = (0.9, 0.999),\n",
    "    eps = 1e-08,\n",
    "    weight_decay = 0,\n",
    "    amsgrad = False\n",
    ")\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(50):\n",
    "    running_loss_train = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs_cpu, targets_cpu = data\n",
    "        inputs_gpu = inputs_cpu.cuda()\n",
    "        targets_gpu = targets_cpu.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs_gpu = net_gpu.train()(inputs_gpu)\n",
    "        loss = loss_func(outputs_gpu, targets_gpu)\n",
    "        running_loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 8 == 7:\n",
    "            print('Train Epoch: %d [%d/5000] Loss: %.6f' %(epoch, i*64, loss.item()))\n",
    "    running_loss_train /= len(trainloader)\n",
    "    print(running_loss_train)\n",
    "    summaryWriter.add_scalar(\"loss\", running_loss_train, epoch)\n",
    "\n",
    "    # Step 4 Predict\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    for data in testloader:\n",
    "        images_cpu, targets_cpu = data\n",
    "        images_gpu = images_cpu.cuda()\n",
    "        targets_gpu = targets_cpu.cuda()\n",
    "        outputs_gpu = net_gpu.eval()(images_gpu)\n",
    "        _, predicted = torch.max(outputs_gpu, 1)\n",
    "        loss = loss_func(outputs_gpu, targets_gpu)\n",
    "        total += targets_gpu.size(0)\n",
    "        running_loss += loss.item()\n",
    "        correct += (predicted == targets_gpu).sum().item()\n",
    "    \n",
    "    running_loss = running_loss / 10000\n",
    "\n",
    "    print('Test set: Average loss: %.4f, Accuracy: %d/10000 (%d%%)' %(running_loss, correct, correct*100/total))\n",
    "    summaryWriter.add_scalar(\"accuracy\", correct/total, epoch)\n",
    "\n",
    "print('Train and predict complete!')    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX1: Conv2D-Conv2D-Conv2D-MaxPooling-Flatten-Dense 98.05%\n",
    "EX2: Conv2D-MaxPooling-Conv2D-MaxPooling-Conv2D-MaxPooling-Flatten-Dense 92%\n",
    "EX3: Conv2D-MaxPooling-Conv2D-MaxPooling-DropOut-Conv2D-MaxPooling-DropOut-Flatten-Dense-Dense-Dense 99%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 3. 3. 2. 2. 0. 3. 2. 3.]\n",
      "[2. 2. 3. 3. 2. 2. 0. 3. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "all_preds=[]\n",
    "all_labels=[]\n",
    "\n",
    "for i,data in enumerate(testloader):\n",
    "    imgs,labels=data\n",
    "    imgs_gpu=imgs.cuda()\n",
    "    labels_gpu=labels.cuda()\n",
    "    all_labels=np.append(all_labels,labels_gpu.cpu().numpy())\n",
    "\n",
    "    output=net_gpu(imgs_gpu)\n",
    "    _, preds = torch.max(output, 1)\n",
    "    all_preds=np.append(all_preds,preds.tolist())\n",
    "\n",
    "print(all_preds[:10])\n",
    "print(all_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = all_preds \n",
    "y_true = all_labels\n",
    "\n",
    "C = confusion_matrix(y_true, y_pred, labels=[0,1,2,3]) \n",
    "plt.matshow(C, cmap=plt.cm.Reds)\n",
    "\n",
    "for i in range(len(C)):\n",
    "    for j in range(len(C)):\n",
    "        plt.annotate(C[j, i], xy=(i, j), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9830293733218458 0.9828125 0.9827953656285455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "precision = precision_score(all_labels,all_preds,average='weighted')\n",
    "recall = recall_score(all_labels,all_preds,average='weighted')\n",
    "f1 = f1_score(all_labels,all_preds,average='weighted')\n",
    "print(precision,recall,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 19 2022, 10:19:43) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
